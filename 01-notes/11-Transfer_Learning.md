# Transfer Learning

在实践中，很少有人从头开始训练整个卷积网络（随机初始化），因为拥有足够大小的数据集是相对罕见的。**相反，通常在非常大的数据集（例如ImageNet，其包含具有1000个类别的120万个图像）上预先训练ConvNet，然后使用ConvNet作为感兴趣的任务的初始化或固定特征提取器。** 三种主要的转学习方案如下

1. ConvNet作为固定特征提取器
    在ImageNet上预先训练一个ConvNet，删除最后一个完全连接的层（该层的输出是ImageNet等不同任务的1000个等级分数），然后将其余的ConvNet视为新数据集的固定特征提取器。在AlexNet中，这将为包含紧接在分类器之前的隐藏层的激活的每个图像计算4096-D向量。我们将这些功能称为**CNN代码**。如果这些代码在ImageNet上的ConvNet训练期间也被阈值处理（通常就是这种情况），那么这些代码是ReLUd（即阈值为零）对性能很重要。为所有图像提取4096-D代码后，为新数据集训练线性分类器（例如线性SVM或Softmax分类器）。
2. 微调ConvNet。
    第二种策略是不仅在新数据集上替换和重新训练ConvNet之上的分类器，而且还通过继续反向传播来微调预训练网络的权重。可以微调ConvNet的所有层，或者可以保留一些早期层（由于过度拟合问题）并且仅微调网络的某些更高级别部分。这是因为观察到ConvNet的早期特征包含更多通用特征（例如边缘检测器或颜色斑点检测器），这些特征应该对许多任务有用，但后来的ConvNet层逐渐变得更加特定于类的细节。包含在原始数据集中。以ImageNet为例，其中包含许多犬种，
3. 预训练模型。
    由于现代需要使用多个GPU花2-3周的时间来训练ImageNet上的ConvNets，因此通常会看到人们发布他们的最终ConvNet检查点，以便其他可以使用网络进行微调的人获益。例如，Caffe库有一个[模型动物园](https://github.com/BVLC/caffe/wiki/Model-Zoo)，人们可以在这里共享网络权重。

## 1. 何时以及如何微调

您如何确定应在新数据集上执行哪种类型的转移学习？这是几个因素的函数，但最重要的两个因素是新数据集的大小（小或大），以及它与原始数据集的相似性（例如ImageNet类似于图像和类的内容，或者非常不同，例如显微镜图像）。请记住，**ConvNet功能在早期层中更通用，而在后续层中更具原始数据集特性**，以下是导航4种主要场景的一些常用经验法则：

1. 新数据集很小，与原始数据集类似。
    由于数据很小，因为过度拟合问题而对ConvNet进行微调并不是一个好主意。由于数据与原始数据类似，我们希望ConvNet中的更高级别功能也与此数据集相关。因此，最好的想法可能是在CNN码上训练线性分类器。
2. 新数据集很大，与原始数据集类似。
    由于我们有更多的数据，我们可以更有信心，如果我们试图通过整个网络进行微调，我们就不会过拟合。
3. 新数据集很小但与原始数据集非常不同。
    由于数据很小，因此最好只训练线性分类器。由于数据集非常不同，因此最好不要从网络顶部训练分类器，其中包含更多**原始数据集特定的功能**。相反，在网络早期的某个地方训练SVM分类器可能会更好。
4. 新数据集很大，与原始数据集非常不同。
    由于数据集非常大，我们可以预期我们可以从头开始训练ConvNet。然而，在实践中，使用来自预训练模型的权重进行初始化通常仍然是有益的。在这种情况下，我们将有足够的数据和信心来微调整个网络。

## 2. 实用建议

在执行转移学习时，还需要注意以下几点:

1. 来自预训练模型的约束。
    请注意，如果您希望使用预训练网络，则可能会对可用于新数据集的体系结构略有限制。例如，您不能随意从预训练网络中取出Conv图层。但是，一些变化是直截了当的：由于参数共享，您可以轻松地在不同空间大小的图像上运行预训练网络。这在Conv / Pool层的情况下是显而易见的，因为它们的前向函数独立于输入体积空间大小（只要步幅“适合”）。在FC层的情况下，这仍然适用，因为FC层可以转换为卷积层：例如，在AlexNet中，第一个FC层之前的最终合并卷的大小为[6x6x512]。因此，
2. 学习率。
    与用于计算新数据集的类别得分的新线性分类器的（随机初始化的）权重相比，通常使用较小的学习速率来对ConvNet权重进行微调。这是因为我们**期望ConvNet权重相对较好，因此我们不希望太快和太多地改变它们**（特别是当它们上面的新线性分类器正在通过随机初始化进行训练时）。
