# 02

Inline Question 4:
When is layer normalization likely to not work well, and why?

Using it in a very deep network
Having a very small dimension of features
Having a high regularization term
Answer:
Layer normalization is likely to not work well when:

2. Having a very small dimension of features

The mean and variance statistics are computed over the features for layer normalization layers. It will be very noisy if you have a very small dimension of features.

3. Having a high regularization term

Any network with a high regularization term will not train well. Indeed, if the regularization term is too high then the optimization algorithm will only try to minimize the regularization penalty, not the objective loss. Thus the algorithm performance will be bad.

On the opposite:

1. Using it in a very deep network

This has no influence has each layer normalization layer is normalizing the output of the affine layers independently.